{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7aafcb2-fa18-4b50-8374-0ee02625aa6f",
   "metadata": {},
   "source": [
    "# 6.1 Reading and Writing Data in Text Format "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7300ba7b-9526-4824-974b-78fe859e45c9",
   "metadata": {},
   "source": [
    "Parsing functions in pandas:\n",
    "- `read_csv`\n",
    "- `read_fwf`\n",
    "- `read_clipboard`\n",
    "- `read_excel`\n",
    "- `read_hdf`\n",
    "- `read_html`\n",
    "- `read_json`\n",
    "- `read_msgpack`\n",
    "- `read_pickle`\n",
    "- `read_sas`\n",
    "- `read_sql`\n",
    "- `read_stata`\n",
    "- `read_feather`\n",
    "\n",
    "With optinal arguments: \n",
    "- Indexing: Can treat one or more columns as the returned DF, and whether to get column names from the file, the user, or not at all\n",
    "- Type Inference and Data Conversion: Includes the **user-defined value conversions** and custom list of missing value markers.\n",
    "- Datetime Parsing: Includes **combining capability**, including combining date and time info spread over multiple columns into a single column in the result.\n",
    "- Iterating: Iterating over chunks of very large files.\n",
    "- Unclean Data Issues: Skipping rows or a footer, comments, or other minor things like numeric data with thousands separated by commas.\n",
    "\n",
    "Some of these functions perform type inference because the column data types are not part of the data format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b0d415b-94fc-43c4-b3ef-43e0d1c7000a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>2</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>foo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a   b   c   2 message\n",
       "0  1   2   3   4   hello\n",
       "1  5   6   7   8   world\n",
       "2  9  10  11  12     foo"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read examples/ex1.csv\n",
    "import pandas as pd \n",
    "df = pd.read_csv('examples/ex1.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8582fc0f-140f-4721-a9f5-aac66f302384",
   "metadata": {},
   "source": [
    "Notice the file has a header row. If the file you are working with does not have one, pass `header=None` **or** assign the names to the columns yourself by passing the `names` argument. \n",
    "- `df = pd.read_csv('examples/ex1.csv, header=None)`\n",
    "- `df = pd.read_csv('examples/ex1.csv, names=['col1','col2', ...])`\n",
    "\n",
    "You can indicate what column you would like to be the index column:\n",
    "- `index_col='col6`\n",
    "\n",
    "Furthermore you can create a hierarchical index (multiple index values) by passing a list of columns to the `index_col` argument. \n",
    "\n",
    "In cases where the data does not have a fixed delimiter, you can pass `sep` argument, and use a **regular expression** to choose the delimeter. `read_csv` can infer which column to be the DF's index. I*t does this by noticing that there is one fewer column name in the data u are passing. \n",
    "\n",
    "Pass a list of indeces to the `skiprows` argument to skip those rows when loading in the data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0558acb-7e64-4b05-9058-09d7aa1aed2f",
   "metadata": {},
   "source": [
    " Handling missing values is an important and frequently nuanced part of the file parsing process. Missing data is usually either not present (empty string) or marked by some **sentinel** value. By default, pandas uses a set of commonly occurring sentinels such as `NA` and `NULL`. \n",
    " - Use `pd.isnull(data_frame)` to return a boolean DF indicating missing values. Furthermore you can use this as a mask :)\n",
    "- Pass a list, dict, or a set of strings to the `na_values` argument to assign missing values.\n",
    "- You can assign different `NA` sentinels to each column, just pass a dict to the `na_values` argument with the column name as the key and the sentinel as the value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9592d3c6-6633-4077-b696-3be43e93a5e1",
   "metadata": {},
   "source": [
    "Common `read_csv` function arguments:\n",
    "- `path`\n",
    "- `sep` or `delimeter`\n",
    "- `header` a row number to use as column names\n",
    "- `index_col`\n",
    "- `names`\n",
    "- `skiprows`\n",
    "- `na_values`\n",
    "- `comment`\n",
    "- `parse_dates`\n",
    "- `keep_date_col`\n",
    "- `converters`\n",
    "- `dayfirst`\n",
    "- `date_parser` a function to use to parse data\n",
    "- `nrows`\n",
    "- `iterator`\n",
    "- `chunksize`\n",
    "- `skip_footer`\n",
    "- `verbose`\n",
    "- `encoding`\n",
    "- `squeeze` Returns a series if the parsed data only contains one column.\n",
    "- `thousands`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d81cdf-bc5f-45eb-ba0f-b7d4d1f5d706",
   "metadata": {},
   "source": [
    "## Reading Text Files in Pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf581b50-76f9-4dc6-a082-48dd7ff567b9",
   "metadata": {},
   "source": [
    "Use the `nrows` argument with `pd.read_csv()` to limit the number of rows to load in. Alternatively, iterate over the file according to chunk size using `chunksize` argument \n",
    "- Create a `TextFileReader` chunk object: `chunker = pd.read_csv('path', cunksize=1000)`\n",
    "- Now use this object to iterate over the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5344c4f-7d45-40c1-be2a-76b9e205703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate the counts in the 'key' column of our data. \n",
    "\n",
    "# total = pd.Series([])\n",
    "# for piece in chunker: \n",
    "#     total.add(piece['key'].value_counts(), fill_value = 0)\n",
    "# total = sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f236e3e1-14a5-47e9-b481-78caf3f5688d",
   "metadata": {},
   "source": [
    "This code returns a series `total` with the index as the column specified `key` and the values are the value counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ce4cc8-db26-43e8-a78e-19f082fa0962",
   "metadata": {},
   "source": [
    "## Writing Data to Text Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2f0b42-fc05-4f25-bfbb-29ac135bee06",
   "metadata": {},
   "source": [
    "Use the `to_csv` method. \n",
    "- `sep` can be used here for delimiter.\n",
    "- By default, missing values appear as empty strings in the output. Pass `na_rep` to change this.\n",
    "- Both the rown and column labels are written by default. This can be disabled with `index=False`, `header=False`.\n",
    "- Write only a subset of the columns by passing a list of column names to `columns` argument\n",
    "- Series also have a `to_csv` method.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2772ad7d-b37c-4c5f-9778-ce466b2bc108",
   "metadata": {},
   "source": [
    "## Working with Delimited Formats "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9df82f-1439-44a6-9492-aedebec1f3fa",
   "metadata": {},
   "source": [
    "For any file with a single-character delimiter, you can use Pythons built-in csv module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e0770ed-c251-4437-bf1f-b1e8328c37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "f = open('examples/ex7.csv')\n",
    "reader = csv.reader(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec78806-d8fa-497e-8a12-3a334949ea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c']\n",
      "['1', '2', '3']\n",
      "['1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "for line in reader:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ff9389-ac93-4a68-9f66-7cae8e63b3cd",
   "metadata": {},
   "source": [
    "Now lets put the data in the form that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17effb32-6466-4299-bcae-84958f685543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': ('1', '1'), 'b': ('2', '2'), 'c': ('3', '3')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('examples/ex7.csv') as f:\n",
    "    lines=list(csv.reader(f))\n",
    "\n",
    "# split the lines into the header line and the data lines:\n",
    "header, values = lines[0], lines[1:]\n",
    "\n",
    "# create a dict of data columns using dict comprehension and the expression zip(*values), which transpose rows to columns. \n",
    "data_dict = {h:v for h, v in zip(header, zip(*values))}\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b961f2-3b8d-4073-adbb-357b30d4dd0d",
   "metadata": {},
   "source": [
    "Since CSV files come in many different flavors, we can define a new format with a different delimiter, string quoting convention, or line terminator. Define a simple subclass of `csv.Dialect`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3265e2-14f1-4e2a-80a0-fc0a9e2e930d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class my_dialect(csv.Dialect):\n",
    "    lineterminator = '\\n'\n",
    "    delimiter = ';'\n",
    "    quotechar = '\"'\n",
    "    quoting = csv.QUOTE_MINIMAL\n",
    "\n",
    "reader = csv.reader(f, dialect = my_dialect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6caf8e-8e5c-405f-a02e-333330d803da",
   "metadata": {},
   "source": [
    "If you dont need to go this far with it, you can simply pass one of these as an argument to `csv.reader`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e384768-64f6-4474-9505-e141b4b67bff",
   "metadata": {},
   "source": [
    "CSV Dialect Options:\n",
    "- `delimiter`\n",
    "- `lineterminator`\n",
    "- `quotechar`\n",
    "- `quoting`\n",
    "- `skipinitialspace`\n",
    "- `doublequote`\n",
    "- `escapechar`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0991443-22fb-42e7-85e6-05a0522b9378",
   "metadata": {},
   "source": [
    "Note: For files with more complicated or fixed multicharacter delimiters, you will not be able to use the `csv` module. In those cases, you will have to do the line splitting and other cleanup using string's `split` method or the regular expression method `re.split`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ac4849-db28-427d-a8ba-fe5e9ef1d480",
   "metadata": {},
   "source": [
    "## JSON Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecdd972-ff57-4d22-acff-f6b58113626f",
   "metadata": {},
   "source": [
    "(JavaScript Object Notation) has become one of the standard formats for sending data by HTTP request between web browsers and other applications. It is a much more free-form data format than a tabular text form like CSV. JSON is nearly python code with the exception of  its null value `null` and some other nuances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a3d298-a9a9-46f3-b20e-1646be8bef95",
   "metadata": {},
   "source": [
    "The basic types are:\n",
    "- objects (dicts)\n",
    "- arrays (lists)\n",
    "- strings\n",
    "- numbers\n",
    "- booleans\n",
    "- nulls\n",
    "\n",
    "Note: **all of these keys in an object must be strings**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deab5a71-bad5-41f2-bbc5-98e0af4766c7",
   "metadata": {},
   "source": [
    "`json` is a Python library for reading and writing JSON data. \n",
    "- To convert a JSON string to Python form, use `json.loads`.\n",
    "- To convert a Python object into JSON format, use `json.dumps`.\n",
    "- You can pass a list of dicts to the DataFrame constructor.\n",
    "- `data_frame = pd.DataFrame(dict_obj['key3'], columns=['col1', 'col2']`\n",
    "- In the above example, we have a nested dict structure, so the columns define the keys that are nested within the `key3` data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aed7b1e-38ab-4ff9-b7e3-55171a70afdb",
   "metadata": {},
   "source": [
    "The `pandas.read_json` can automatically convert JSON datasets in specific arrangements into a Series or DatFrame. \n",
    "- The default option for this method is to **assume that each object in the JSON array is a row in the table**.\n",
    "\n",
    "If you need to export data from pandas to JSON, one way is to use the `to_json` methods on Series and DataFrame. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89996934-5b15-4fd8-a165-dfa99be8a1ed",
   "metadata": {},
   "source": [
    "## XML and HTML: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a70b9a-7751-4c88-b04e-5fe6b12da3df",
   "metadata": {},
   "source": [
    "Examples of libraries that support this: \n",
    "- `lxml`\n",
    "- `beautifulsoup`\n",
    "- `html5lib`\n",
    "\n",
    "Pandas has a built-in function, `read_html` which uses those libraries automatically to parse tables out of HTML files as DF objects. \n",
    "- By default, it searches for and attempts to parse all tabular data contained within `<table>` tags. The result is a list of DF objects.\n",
    "- `tables = pd.read_html('dir/file.html')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1943092-c474-4003-a2d4-c3f02e7101d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables = pd.read_html('fdic_failed_bank_list.html')\n",
    "len(tables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cd6b786-65b3-47ee-8eed-8bfa1d6117d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "failures=tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fae3f88a-e81a-4ed5-b319-9e718dfcd0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bank Name</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Cert</th>\n",
       "      <th>Acquiring Institution</th>\n",
       "      <th>Closing Date</th>\n",
       "      <th>Fund  Sort ascending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pulaski Savings Bank</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>28611</td>\n",
       "      <td>Millennium Bank</td>\n",
       "      <td>January 17, 2025</td>\n",
       "      <td>10548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The First National Bank of Lindsay</td>\n",
       "      <td>Lindsay</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>4134</td>\n",
       "      <td>First Bank &amp; Trust Co., Duncan, OK</td>\n",
       "      <td>October 18, 2024</td>\n",
       "      <td>10547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Republic First Bank dba Republic Bank</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>27332</td>\n",
       "      <td>Fulton Bank, National Association</td>\n",
       "      <td>April 26, 2024</td>\n",
       "      <td>10546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Citizens Bank</td>\n",
       "      <td>Sac City</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>8758</td>\n",
       "      <td>Iowa Trust &amp; Savings Bank</td>\n",
       "      <td>November 3, 2023</td>\n",
       "      <td>10545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Heartland Tri-State Bank</td>\n",
       "      <td>Elkhart</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>25851</td>\n",
       "      <td>Dream First Bank, N.A.</td>\n",
       "      <td>July 28, 2023</td>\n",
       "      <td>10544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Bank Name          City         State   Cert  \\\n",
       "0                   Pulaski Savings Bank       Chicago      Illinois  28611   \n",
       "1     The First National Bank of Lindsay       Lindsay      Oklahoma   4134   \n",
       "2  Republic First Bank dba Republic Bank  Philadelphia  Pennsylvania  27332   \n",
       "3                          Citizens Bank      Sac City          Iowa   8758   \n",
       "4               Heartland Tri-State Bank       Elkhart        Kansas  25851   \n",
       "\n",
       "                Acquiring Institution      Closing Date  Fund  Sort ascending  \n",
       "0                     Millennium Bank  January 17, 2025                 10548  \n",
       "1  First Bank & Trust Co., Duncan, OK  October 18, 2024                 10547  \n",
       "2   Fulton Bank, National Association    April 26, 2024                 10546  \n",
       "3           Iowa Trust & Savings Bank  November 3, 2023                 10545  \n",
       "4              Dream First Bank, N.A.     July 28, 2023                 10544  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d1c0e5d-2c26-41d5-abb3-8b50c3ba910c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2025-01-17\n",
       "1   2024-10-18\n",
       "2   2024-04-26\n",
       "3   2023-11-03\n",
       "4   2023-07-28\n",
       "5   2023-05-01\n",
       "6   2023-03-12\n",
       "7   2023-03-10\n",
       "8   2020-10-23\n",
       "9   2020-10-16\n",
       "Name: Closing Date, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute number of bank failures by year \n",
    "close_timestamps = pd.to_datetime(failures['Closing Date'])\n",
    "close_timestamps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7db0d524-70a1-4ec9-aebb-d5dbc9811774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Closing Date\n",
       "10    3\n",
       "3     2\n",
       "1     1\n",
       "4     1\n",
       "11    1\n",
       "7     1\n",
       "5     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_timestamps.dt.month.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18798c30-094f-4266-947e-a8143cf73199",
   "metadata": {},
   "source": [
    "Since the object `close_timestamps` is of `dtype` `datetime64`, it has attributes such as `.dt.year`, and `.dt.month`, .etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4319f93f-d011-4213-95a5-c3565341f97b",
   "metadata": {},
   "source": [
    "### Parsing XML with `lxml.objectify`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37c32df-5ae9-4b1a-9736-cbf933cd161e",
   "metadata": {},
   "source": [
    "**(eXtensible Markup Language)** is another common structured data format supporting hierarchical, nested data with metadata. \n",
    "- XML and HTML are structured similarly, but XML is more general. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0455d2-c2a7-40fe-a1a4-7b9356d7d194",
   "metadata": {},
   "source": [
    "Using `lxml.objectify`, we parse the file and get a reference to the root node of the XML file wth `getroot`\n",
    "- `from lxml import objectify` \n",
    "- `path = 'path_to_xml'`\n",
    "- `parsed = objectify.parse(open(path))`\n",
    "- `root = parsed.getroot()`\n",
    "\n",
    "`root.INDICATOR` returns a generator yielding each `<INDICATOR>` XML element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381e18c-8a8f-44d0-be81-e576348d9742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe6690-b6f4-4203-a421-f1ebefc32b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b0bf3-397b-4937-adb2-67a9478059b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fc2796-a28e-4ca9-b037-3f3618f2f636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e334e33-93fd-45e7-ab7f-6b68680ee012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14e0690-0625-4cea-9da7-a50e760468c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5a9f0-43f9-492f-bbed-b682e2c1b27e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2707f8-2874-451f-959f-b33edbd2d558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
