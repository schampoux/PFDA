{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7aafcb2-fa18-4b50-8374-0ee02625aa6f",
   "metadata": {},
   "source": [
    "# 6.1 Reading and Writing Data in Text Format "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7300ba7b-9526-4824-974b-78fe859e45c9",
   "metadata": {},
   "source": [
    "Parsing functions in pandas:\n",
    "- `read_csv`\n",
    "- `read_fwf`\n",
    "- `read_clipboard`\n",
    "- `read_excel`\n",
    "- `read_hdf`\n",
    "- `read_html`\n",
    "- `read_json`\n",
    "- `read_msgpack`\n",
    "- `read_pickle`\n",
    "- `read_sas`\n",
    "- `read_sql`\n",
    "- `read_stata`\n",
    "- `read_feather`\n",
    "\n",
    "With optinal arguments: \n",
    "- Indexing: Can treat one or more columns as the returned DF, and whether to get column names from the file, the user, or not at all\n",
    "- Type Inference and Data Conversion: Includes the **user-defined value conversions** and custom list of missing value markers.\n",
    "- Datetime Parsing: Includes **combining capability**, including combining date and time info spread over multiple columns into a single column in the result.\n",
    "- Iterating: Iterating over chunks of very large files.\n",
    "- Unclean Data Issues: Skipping rows or a footer, comments, or other minor things like numeric data with thousands separated by commas.\n",
    "\n",
    "Some of these functions perform type inference because the column data types are not part of the data format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b0d415b-94fc-43c4-b3ef-43e0d1c7000a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>2</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>foo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a   b   c   2 message\n",
       "0  1   2   3   4   hello\n",
       "1  5   6   7   8   world\n",
       "2  9  10  11  12     foo"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read examples/ex1.csv\n",
    "import pandas as pd \n",
    "df = pd.read_csv('examples/ex1.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8582fc0f-140f-4721-a9f5-aac66f302384",
   "metadata": {},
   "source": [
    "Notice the file has a header row. If the file you are working with does not have one, pass `header=None` **or** assign the names to the columns yourself by passing the `names` argument. \n",
    "- `df = pd.read_csv('examples/ex1.csv, header=None)`\n",
    "- `df = pd.read_csv('examples/ex1.csv, names=['col1','col2', ...])`\n",
    "\n",
    "You can indicate what column you would like to be the index column:\n",
    "- `index_col='col6`\n",
    "\n",
    "Furthermore you can create a hierarchical index (multiple index values) by passing a list of columns to the `index_col` argument. \n",
    "\n",
    "In cases where the data does not have a fixed delimiter, you can pass `sep` argument, and use a **regular expression** to choose the delimeter. `read_csv` can infer which column to be the DF's index. I*t does this by noticing that there is one fewer column name in the data u are passing. \n",
    "\n",
    "Pass a list of indeces to the `skiprows` argument to skip those rows when loading in the data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0558acb-7e64-4b05-9058-09d7aa1aed2f",
   "metadata": {},
   "source": [
    " Handling missing values is an important and frequently nuanced part of the file parsing process. Missing data is usually either not present (empty string) or marked by some **sentinel** value. By default, pandas uses a set of commonly occurring sentinels such as `NA` and `NULL`. \n",
    " - Use `pd.isnull(data_frame)` to return a boolean DF indicating missing values. Furthermore you can use this as a mask :)\n",
    "- Pass a list, dict, or a set of strings to the `na_values` argument to assign missing values.\n",
    "- You can assign different `NA` sentinels to each column, just pass a dict to the `na_values` argument with the column name as the key and the sentinel as the value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9592d3c6-6633-4077-b696-3be43e93a5e1",
   "metadata": {},
   "source": [
    "Common `read_csv` function arguments:\n",
    "- `path`\n",
    "- `sep` or `delimeter`\n",
    "- `header` a row number to use as column names\n",
    "- `index_col`\n",
    "- `names`\n",
    "- `skiprows`\n",
    "- `na_values`\n",
    "- `comment`\n",
    "- `parse_dates`\n",
    "- `keep_date_col`\n",
    "- `converters`\n",
    "- `dayfirst`\n",
    "- `date_parser` a function to use to parse data\n",
    "- `nrows`\n",
    "- `iterator`\n",
    "- `chunksize`\n",
    "- `skip_footer`\n",
    "- `verbose`\n",
    "- `encoding`\n",
    "- `squeeze` Returns a series if the parsed data only contains one column.\n",
    "- `thousands`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d81cdf-bc5f-45eb-ba0f-b7d4d1f5d706",
   "metadata": {},
   "source": [
    "## Reading Text Files in Pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf581b50-76f9-4dc6-a082-48dd7ff567b9",
   "metadata": {},
   "source": [
    "Use the `nrows` argument with `pd.read_csv()` to limit the number of rows to load in. Alternatively, iterate over the file according to chunk size using `chunksize` argument \n",
    "- Create a `TextFileReader` chunk object: `chunker = pd.read_csv('path', cunksize=1000)`\n",
    "- Now use this object to iterate over the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5344c4f-7d45-40c1-be2a-76b9e205703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate the counts in the 'key' column of our data. \n",
    "\n",
    "# total = pd.Series([])\n",
    "# for piece in chunker: \n",
    "#     total.add(piece['key'].value_counts(), fill_value = 0)\n",
    "# total = sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f236e3e1-14a5-47e9-b481-78caf3f5688d",
   "metadata": {},
   "source": [
    "This code returns a series `total` with the index as the column specified `key` and the values are the value counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ce4cc8-db26-43e8-a78e-19f082fa0962",
   "metadata": {},
   "source": [
    "## Writing Data to Text Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2f0b42-fc05-4f25-bfbb-29ac135bee06",
   "metadata": {},
   "source": [
    "Use the `to_csv` method. \n",
    "- `sep` can be used here for delimiter.\n",
    "- By default, missing values appear as empty strings in the output. Pass `na_rep` to change this.\n",
    "- Both the rown and column labels are written by default. This can be disabled with `index=False`, `header=False`.\n",
    "- Write only a subset of the columns by passing a list of column names to `columns` argument\n",
    "- Series also have a `to_csv` method.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2772ad7d-b37c-4c5f-9778-ce466b2bc108",
   "metadata": {},
   "source": [
    "## Working with Delimited Formats "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9df82f-1439-44a6-9492-aedebec1f3fa",
   "metadata": {},
   "source": [
    "For any file with a single-character delimiter, you can use Pythons built-in csv module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e0770ed-c251-4437-bf1f-b1e8328c37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "f = open('examples/ex7.csv')\n",
    "reader = csv.reader(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec78806-d8fa-497e-8a12-3a334949ea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c']\n",
      "['1', '2', '3']\n",
      "['1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "for line in reader:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ff9389-ac93-4a68-9f66-7cae8e63b3cd",
   "metadata": {},
   "source": [
    "Now lets put the data in the form that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17effb32-6466-4299-bcae-84958f685543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': ('1', '1'), 'b': ('2', '2'), 'c': ('3', '3')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('examples/ex7.csv') as f:\n",
    "    lines=list(csv.reader(f))\n",
    "\n",
    "# split the lines into the header line and the data lines:\n",
    "header, values = lines[0], lines[1:]\n",
    "\n",
    "# create a dict of data columns using dict comprehension and the expression zip(*values), which transpose rows to columns. \n",
    "data_dict = {h:v for h, v in zip(header, zip(*values))}\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b961f2-3b8d-4073-adbb-357b30d4dd0d",
   "metadata": {},
   "source": [
    "Since CSV files come in many different flavors, we can define a new format with a different delimiter, string quoting convention, or line terminator. Define a simple subclass of `csv.Dialect`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3265e2-14f1-4e2a-80a0-fc0a9e2e930d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class my_dialect(csv.Dialect):\n",
    "    lineterminator = '\\n'\n",
    "    delimiter = ';'\n",
    "    quotechar = '\"'\n",
    "    quoting = csv.QUOTE_MINIMAL\n",
    "\n",
    "reader = csv.reader(f, dialect = my_dialect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6caf8e-8e5c-405f-a02e-333330d803da",
   "metadata": {},
   "source": [
    "If you dont need to go this far with it, you can simply pass one of these as an argument to `csv.reader`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e384768-64f6-4474-9505-e141b4b67bff",
   "metadata": {},
   "source": [
    "CSV Dialect Options:\n",
    "- `delimiter`\n",
    "- `lineterminator`\n",
    "- `quotechar`\n",
    "- `quoting`\n",
    "- `skipinitialspace`\n",
    "- `doublequote`\n",
    "- `escapechar`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0991443-22fb-42e7-85e6-05a0522b9378",
   "metadata": {},
   "source": [
    "Note: For files with more complicated or fixed multicharacter delimiters, you will not be able to use the `csv` module. In those cases, you will have to do the line splitting and other cleanup using string's `split` method or the regular expression method `re.split`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ac4849-db28-427d-a8ba-fe5e9ef1d480",
   "metadata": {},
   "source": [
    "## JSON Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecdd972-ff57-4d22-acff-f6b58113626f",
   "metadata": {},
   "source": [
    "(JavaScript Object Notation) has become one of the standard formats for sending data by HTTP request between web browsers and other applications. It is a much more free-form data format than a tabular text form like CSV. JSON is nearly python code with the exception of  its null value `null` and some other nuances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a3d298-a9a9-46f3-b20e-1646be8bef95",
   "metadata": {},
   "source": [
    "The basic types are:\n",
    "- objects (dicts)\n",
    "- arrays (lists)\n",
    "- strings\n",
    "- numbers\n",
    "- booleans\n",
    "- nulls\n",
    "\n",
    "Note: **all of these keys in an object must be strings**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deab5a71-bad5-41f2-bbc5-98e0af4766c7",
   "metadata": {},
   "source": [
    "`json` is a Python library for reading and writing JSON data. \n",
    "- To convert a JSON string to Python form, use `json.loads`.\n",
    "- To convert a Python object into JSON format, use `json.dumps`.\n",
    "- You can pass a list of dicts to the DataFrame constructor.\n",
    "- `data_frame = pd.DataFrame(dict_obj['key3'], columns=['col1', 'col2']`\n",
    "- In the above example, we have a nested dict structure, so the columns define the keys that are nested within the `key3` data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aed7b1e-38ab-4ff9-b7e3-55171a70afdb",
   "metadata": {},
   "source": [
    "The `pandas.read_json` can automatically convert JSON datasets in specific arrangements into a Series or DatFrame. \n",
    "- The default option for this method is to **assume that each object in the JSON array is a row in the table**.\n",
    "\n",
    "If you need to export data from pandas to JSON, one way is to use the `to_json` methods on Series and DataFrame. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89996934-5b15-4fd8-a165-dfa99be8a1ed",
   "metadata": {},
   "source": [
    "## XML and HTML: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a70b9a-7751-4c88-b04e-5fe6b12da3df",
   "metadata": {},
   "source": [
    "Examples of libraries that support this: \n",
    "- `lxml`\n",
    "- `beautifulsoup`\n",
    "- `html5lib`\n",
    "\n",
    "Pandas has a built-in function, `read_html` which uses those libraries automatically to parse tables out of HTML files as DF objects. \n",
    "- By default, it searches for and attempts to parse all tabular data contained within `<table>` tags. The result is a list of DF objects.\n",
    "- `tables = pd.read_html('dir/file.html')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1943092-c474-4003-a2d4-c3f02e7101d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables = pd.read_html('fdic_failed_bank_list.html')\n",
    "len(tables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cd6b786-65b3-47ee-8eed-8bfa1d6117d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "failures=tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fae3f88a-e81a-4ed5-b319-9e718dfcd0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bank Name</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Cert</th>\n",
       "      <th>Acquiring Institution</th>\n",
       "      <th>Closing Date</th>\n",
       "      <th>Fund  Sort ascending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pulaski Savings Bank</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>28611</td>\n",
       "      <td>Millennium Bank</td>\n",
       "      <td>January 17, 2025</td>\n",
       "      <td>10548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The First National Bank of Lindsay</td>\n",
       "      <td>Lindsay</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>4134</td>\n",
       "      <td>First Bank &amp; Trust Co., Duncan, OK</td>\n",
       "      <td>October 18, 2024</td>\n",
       "      <td>10547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Republic First Bank dba Republic Bank</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>27332</td>\n",
       "      <td>Fulton Bank, National Association</td>\n",
       "      <td>April 26, 2024</td>\n",
       "      <td>10546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Citizens Bank</td>\n",
       "      <td>Sac City</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>8758</td>\n",
       "      <td>Iowa Trust &amp; Savings Bank</td>\n",
       "      <td>November 3, 2023</td>\n",
       "      <td>10545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Heartland Tri-State Bank</td>\n",
       "      <td>Elkhart</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>25851</td>\n",
       "      <td>Dream First Bank, N.A.</td>\n",
       "      <td>July 28, 2023</td>\n",
       "      <td>10544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Bank Name          City         State   Cert  \\\n",
       "0                   Pulaski Savings Bank       Chicago      Illinois  28611   \n",
       "1     The First National Bank of Lindsay       Lindsay      Oklahoma   4134   \n",
       "2  Republic First Bank dba Republic Bank  Philadelphia  Pennsylvania  27332   \n",
       "3                          Citizens Bank      Sac City          Iowa   8758   \n",
       "4               Heartland Tri-State Bank       Elkhart        Kansas  25851   \n",
       "\n",
       "                Acquiring Institution      Closing Date  Fund  Sort ascending  \n",
       "0                     Millennium Bank  January 17, 2025                 10548  \n",
       "1  First Bank & Trust Co., Duncan, OK  October 18, 2024                 10547  \n",
       "2   Fulton Bank, National Association    April 26, 2024                 10546  \n",
       "3           Iowa Trust & Savings Bank  November 3, 2023                 10545  \n",
       "4              Dream First Bank, N.A.     July 28, 2023                 10544  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d1c0e5d-2c26-41d5-abb3-8b50c3ba910c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2025-01-17\n",
       "1   2024-10-18\n",
       "2   2024-04-26\n",
       "3   2023-11-03\n",
       "4   2023-07-28\n",
       "5   2023-05-01\n",
       "6   2023-03-12\n",
       "7   2023-03-10\n",
       "8   2020-10-23\n",
       "9   2020-10-16\n",
       "Name: Closing Date, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute number of bank failures by year \n",
    "close_timestamps = pd.to_datetime(failures['Closing Date'])\n",
    "close_timestamps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7db0d524-70a1-4ec9-aebb-d5dbc9811774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Closing Date\n",
       "10    3\n",
       "3     2\n",
       "1     1\n",
       "4     1\n",
       "11    1\n",
       "7     1\n",
       "5     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_timestamps.dt.month.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18798c30-094f-4266-947e-a8143cf73199",
   "metadata": {},
   "source": [
    "Since the object `close_timestamps` is of `dtype` `datetime64`, it has attributes such as `.dt.year`, and `.dt.month`, .etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4319f93f-d011-4213-95a5-c3565341f97b",
   "metadata": {},
   "source": [
    "### Parsing XML with `lxml.objectify`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37c32df-5ae9-4b1a-9736-cbf933cd161e",
   "metadata": {},
   "source": [
    "**(eXtensible Markup Language)** is another common structured data format supporting hierarchical, nested data with metadata. \n",
    "- XML and HTML are structured similarly, but XML is more general. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0455d2-c2a7-40fe-a1a4-7b9356d7d194",
   "metadata": {},
   "source": [
    "Using `lxml.objectify`, we parse the file and get a reference to the root node of the XML file wth `getroot`\n",
    "- `from lxml import objectify` \n",
    "- `path = 'path_to_xml'`\n",
    "- `parsed = objectify.parse(open(path))`\n",
    "- `root = parsed.getroot()`\n",
    "\n",
    "`root.INDICATOR` returns a generator yielding each `<INDICATOR>` XML element. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bbe71e-35cc-4af8-8513-261a981c5eba",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc851ce-a26a-4336-97de-17fd552ee070",
   "metadata": {},
   "source": [
    "# 6.2 Binary Data Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c0fb77-660c-44b6-845a-d75cdf136f2b",
   "metadata": {},
   "source": [
    "**serialization** is another word for storing data. \n",
    "\n",
    "One of the easiest ways to store data efficiently in binary is using Pythons built-in pickle serialization. \n",
    "- Pandas objects have a `to_pickle` method the writes data to disk in pickle format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39e5cbcf-9538-4a2e-87bb-057bb7737a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>2</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>foo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a   b   c   2 message\n",
       "0  1   2   3   4   hello\n",
       "1  5   6   7   8   world\n",
       "2  9  10  11  12     foo"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load a DF from a csv file\n",
    "frame = pd.read_csv('examples/ex1.csv')\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b5ca313-df63-46fc-b962-bcbf0ad4083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickle in the same dir\n",
    "frame.to_pickle('examples/frame_pickle') # writes to a new file called 'frame_pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1fdc536-bd44-48d0-a624-447740c5eca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>2</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>foo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a   b   c   2 message\n",
       "0  1   2   3   4   hello\n",
       "1  5   6   7   8   world\n",
       "2  9  10  11  12     foo"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read it back \n",
    "pd.read_pickle('examples/frame_pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078dd579-5e10-429f-82a2-2113982bbcdc",
   "metadata": {},
   "source": [
    "Note: Pickle is only recommended as a short term storage format. A file pickled today may not unpicle with a later version of a library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c3afe-5a07-4405-ba05-cf2a1f55ea18",
   "metadata": {},
   "source": [
    "Pandas also has support for two more binary data formats:\n",
    "- HDF5\n",
    "- Message-Pack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cde4ef-c516-4ecc-8925-de158a93859a",
   "metadata": {},
   "source": [
    "## Using HDF5 Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5860e13-3ff5-4334-a66c-aaede00d6473",
   "metadata": {},
   "source": [
    "Well-regarded file format intended for storing large qty's of scientific array data. \n",
    "- **\"Hierarchical Data Format\"**\n",
    "- Each HDF5 file can store multiple datasets and supporting metadata.\n",
    "- Supports on the fly compression with a variety of compression modes, enabling data with repeated patterns to be stored more efficiently.\n",
    "- Good choice for large datasets that dont fit into memory.\n",
    "- Pandas has a `HDF5Store` class that works like a dict and handles the low-level details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6133897a-8995-4502-ae7c-b928bbcf343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# frame = pd.DataFrame({'a': np.random.randn(100)})\n",
    "# store = pd.HDFStore('mydata.h5')\n",
    "# store['obj1'] = frame \n",
    "# store['obj1_col'] = frame['a']\n",
    "# store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7adf5b-0562-4c75-8587-05400d15689e",
   "metadata": {},
   "source": [
    "Objects contained in the HDF5 file cna then be retreived with the same dict-like API\n",
    "- `store['obj1']`\n",
    "\n",
    "HDF5 supports two storage schemas, `fixed` and `table`. The latter is generally slower, but it supports query operations using a special syntax. \n",
    "- `store.put('obj2', frame, format='table')` An explicit version of the `store['obj2']=frame` method but allows us to set other options like the storage format. \n",
    "- `store.select('obj2', where=['index >=10 and index <=15'])`\n",
    "\n",
    "The `pandas.read_hdf` gives us a shortcut to these tools. \n",
    "- `frame.to_hdf('mydata.h5', 'obj3', 'format='table')`\n",
    "- `pd.read_hdf('mydata.h5', 'obj3', where = ['index < 5'])` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a200720d-f3a8-4677-9aa4-36941b4d4f66",
   "metadata": {},
   "source": [
    "If you are working with large quantities of data locally, i would encourage you to explore PyTables and h5py to see how they can suit your needs. Since many data analysis problems are I/O bound rather than CPU bound, using a tool like HDF5 can massively accelerate your applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095e67d2-7cf2-468d-af1c-2c4789bef7d4",
   "metadata": {},
   "source": [
    "### Reading Microsoft Excel Files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b99fd0d-8f54-48bf-9701-f510c65c63fd",
   "metadata": {},
   "source": [
    "(tabular data) using either the `ExcelFile` class or `pandas.read_excel` function. \n",
    "- Internally these tools use the add-on packages `xlrd` and `openpyxl` to read XLS and XLSX files.\n",
    "- `xlsx_file = pd.ExcelFile('examples/ex1.xlsx')`\n",
    "- `pd.read_excel(xlsx_file, 'Sheet1')`\n",
    "\n",
    "Or you can do it in one step with: \n",
    "- `xlsx_file = pd.ExcelFile('path/to/file.xlsx', 'Sheet1')`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb8b976-03cf-44e5-9375-25b14ffbad64",
   "metadata": {},
   "source": [
    "To write pandas data to Excel format, you first make `EscelWriter` object, then write data to it using pandas object method `to_excel`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475dfc75-109f-4ffa-8fd2-75183229ed99",
   "metadata": {},
   "source": [
    "# 6.3 Interact With Web API's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabb4bcf-8bd2-4a03-a98d-18f2dafad2eb",
   "metadata": {},
   "source": [
    "Many websites have public api's providing data feeds via JSON or some other format. There are a number of ways to access these API's from Python. One is the `requests` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb32aed5-1b51-4fee-80b3-5af76c91c53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the last 30 GH issues for pandas on GH \n",
    "import requests\n",
    "url='https://api.github.com/repos/pandas-dev/pandas/issues'\n",
    "resp = requests.get(url)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1cee29-bae2-416e-8be5-e8b84e272d96",
   "metadata": {},
   "source": [
    "The response objects json method will return a dictionary containing JSON parsed into native Python Objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2429a0e2-9bf8-40a7-b3ae-fecaa15a642b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ENH: Create infrastructure for translations'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = resp.json()\n",
    "data[0]['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e91fc55-fa6e-4765-8748-46281f52d717",
   "metadata": {},
   "source": [
    " Each element in `data` is a dict containing all of the data found on a GH issue page (except for the comments). We can pass `data` directly to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "125be5f8-08ab-440e-8f93-d20d9ca80624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>title</th>\n",
       "      <th>labels</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61220</td>\n",
       "      <td>ENH: Create infrastructure for translations</td>\n",
       "      <td>[]</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61219</td>\n",
       "      <td>Fix #58421: Index[timestamp[pyarrow]].union wi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61218</td>\n",
       "      <td>QST: Should the absence of tzdata package affe...</td>\n",
       "      <td>[{'id': 34444536, 'node_id': 'MDU6TGFiZWwzNDQ0...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61217</td>\n",
       "      <td>BUG: unstack incorrectly reshuffles data when ...</td>\n",
       "      <td>[{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61216</td>\n",
       "      <td>BUG: OverflowError when fillna on DataFrame wi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>61214</td>\n",
       "      <td>Restrict clipping of DataFrame.corr only when ...</td>\n",
       "      <td>[{'id': 5331296438, 'node_id': 'LA_kwDOAA0YD88...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>61210</td>\n",
       "      <td>ENH: Add ignore_empty and ignore_all_na argume...</td>\n",
       "      <td>[{'id': 76812, 'node_id': 'MDU6TGFiZWw3NjgxMg=...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>61209</td>\n",
       "      <td>ENH: Consistent NA handling in `unique()`, and...</td>\n",
       "      <td>[{'id': 76812, 'node_id': 'MDU6TGFiZWw3NjgxMg=...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>61208</td>\n",
       "      <td>BUG: OverflowError when fillna on DataFrame wi...</td>\n",
       "      <td>[{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>61206</td>\n",
       "      <td>BUG: round on object columns no longer raises ...</td>\n",
       "      <td>[{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>61204</td>\n",
       "      <td>BUG: DataFrame.min with skipna=True raises Typ...</td>\n",
       "      <td>[{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>61203</td>\n",
       "      <td>BUG: fix to_json on period</td>\n",
       "      <td>[]</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>61195</td>\n",
       "      <td>DOC: User Guide Page on user-defined functions</td>\n",
       "      <td>[{'id': 134699, 'node_id': 'MDU6TGFiZWwxMzQ2OT...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>61194</td>\n",
       "      <td>ENH: adding a filter (and bold) to header when...</td>\n",
       "      <td>[{'id': 76812, 'node_id': 'MDU6TGFiZWw3NjgxMg=...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>61193</td>\n",
       "      <td>BUG: Fix pyarrow categoricals not working for ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>61191</td>\n",
       "      <td>BUG: Boolean selection edge case.</td>\n",
       "      <td>[{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>61188</td>\n",
       "      <td>BUG: date comparison fails when series is all ...</td>\n",
       "      <td>[{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>61186</td>\n",
       "      <td>BUG: engine calamine lost 0 when read_excel fr...</td>\n",
       "      <td>[{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>61185</td>\n",
       "      <td>ENH: Reimplement DataFrame.lookup</td>\n",
       "      <td>[{'id': 76812, 'node_id': 'MDU6TGFiZWw3NjgxMg=...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>61184</td>\n",
       "      <td>Doc pivot table</td>\n",
       "      <td>[{'id': 134699, 'node_id': 'MDU6TGFiZWwxMzQ2OT...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>61182</td>\n",
       "      <td>BUG: Negation of `.str.isnumeric()` changes `d...</td>\n",
       "      <td>[{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>61181</td>\n",
       "      <td>update offsets.pyx to fix #60647</td>\n",
       "      <td>[]</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>61180</td>\n",
       "      <td>BUG: Performance issue with fillna() after mer...</td>\n",
       "      <td>[{'id': 2822342, 'node_id': 'MDU6TGFiZWwyODIyM...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>61177</td>\n",
       "      <td>BUG: Inconsistency when converting week Period...</td>\n",
       "      <td>[{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>61173</td>\n",
       "      <td>BUG: Handle line and bar on the same plot</td>\n",
       "      <td>[{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>61172</td>\n",
       "      <td>BUG: pd.read_csv() on empty files should give ...</td>\n",
       "      <td>[{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>61171</td>\n",
       "      <td>DataFrame.dtypes.to_json OverflowError: Maximu...</td>\n",
       "      <td>[{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>61170</td>\n",
       "      <td>BUG: DataFrame.dtypes.to_json OverflowError: M...</td>\n",
       "      <td>[{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>61168</td>\n",
       "      <td>ENH: Accept no fields for groupby by</td>\n",
       "      <td>[]</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>61166</td>\n",
       "      <td>ENH: Enable nsmallest/nlargest on object dtype</td>\n",
       "      <td>[{'id': 76812, 'node_id': 'MDU6TGFiZWw3NjgxMg=...</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    number                                              title  \\\n",
       "0    61220        ENH: Create infrastructure for translations   \n",
       "1    61219  Fix #58421: Index[timestamp[pyarrow]].union wi...   \n",
       "2    61218  QST: Should the absence of tzdata package affe...   \n",
       "3    61217  BUG: unstack incorrectly reshuffles data when ...   \n",
       "4    61216  BUG: OverflowError when fillna on DataFrame wi...   \n",
       "5    61214  Restrict clipping of DataFrame.corr only when ...   \n",
       "6    61210  ENH: Add ignore_empty and ignore_all_na argume...   \n",
       "7    61209  ENH: Consistent NA handling in `unique()`, and...   \n",
       "8    61208  BUG: OverflowError when fillna on DataFrame wi...   \n",
       "9    61206  BUG: round on object columns no longer raises ...   \n",
       "10   61204  BUG: DataFrame.min with skipna=True raises Typ...   \n",
       "11   61203                         BUG: fix to_json on period   \n",
       "12   61195     DOC: User Guide Page on user-defined functions   \n",
       "13   61194  ENH: adding a filter (and bold) to header when...   \n",
       "14   61193  BUG: Fix pyarrow categoricals not working for ...   \n",
       "15   61191                  BUG: Boolean selection edge case.   \n",
       "16   61188  BUG: date comparison fails when series is all ...   \n",
       "17   61186  BUG: engine calamine lost 0 when read_excel fr...   \n",
       "18   61185                  ENH: Reimplement DataFrame.lookup   \n",
       "19   61184                                    Doc pivot table   \n",
       "20   61182  BUG: Negation of `.str.isnumeric()` changes `d...   \n",
       "21   61181                   update offsets.pyx to fix #60647   \n",
       "22   61180  BUG: Performance issue with fillna() after mer...   \n",
       "23   61177  BUG: Inconsistency when converting week Period...   \n",
       "24   61173          BUG: Handle line and bar on the same plot   \n",
       "25   61172  BUG: pd.read_csv() on empty files should give ...   \n",
       "26   61171  DataFrame.dtypes.to_json OverflowError: Maximu...   \n",
       "27   61170  BUG: DataFrame.dtypes.to_json OverflowError: M...   \n",
       "28   61168               ENH: Accept no fields for groupby by   \n",
       "29   61166     ENH: Enable nsmallest/nlargest on object dtype   \n",
       "\n",
       "                                               labels state  \n",
       "0                                                  []  open  \n",
       "1                                                  []  open  \n",
       "2   [{'id': 34444536, 'node_id': 'MDU6TGFiZWwzNDQ0...  open  \n",
       "3   [{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...  open  \n",
       "4                                                  []  open  \n",
       "5   [{'id': 5331296438, 'node_id': 'LA_kwDOAA0YD88...  open  \n",
       "6   [{'id': 76812, 'node_id': 'MDU6TGFiZWw3NjgxMg=...  open  \n",
       "7   [{'id': 76812, 'node_id': 'MDU6TGFiZWw3NjgxMg=...  open  \n",
       "8   [{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...  open  \n",
       "9   [{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...  open  \n",
       "10  [{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...  open  \n",
       "11                                                 []  open  \n",
       "12  [{'id': 134699, 'node_id': 'MDU6TGFiZWwxMzQ2OT...  open  \n",
       "13  [{'id': 76812, 'node_id': 'MDU6TGFiZWw3NjgxMg=...  open  \n",
       "14                                                 []  open  \n",
       "15  [{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...  open  \n",
       "16  [{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...  open  \n",
       "17  [{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...  open  \n",
       "18  [{'id': 76812, 'node_id': 'MDU6TGFiZWw3NjgxMg=...  open  \n",
       "19  [{'id': 134699, 'node_id': 'MDU6TGFiZWwxMzQ2OT...  open  \n",
       "20  [{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...  open  \n",
       "21                                                 []  open  \n",
       "22  [{'id': 2822342, 'node_id': 'MDU6TGFiZWwyODIyM...  open  \n",
       "23  [{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...  open  \n",
       "24  [{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...  open  \n",
       "25  [{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...  open  \n",
       "26  [{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...  open  \n",
       "27  [{'id': 76811, 'node_id': 'MDU6TGFiZWw3NjgxMQ=...  open  \n",
       "28                                                 []  open  \n",
       "29  [{'id': 76812, 'node_id': 'MDU6TGFiZWw3NjgxMg=...  open  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues = pd.DataFrame(data, columns=['number','title','labels','state'])\n",
    "issues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fc2796-a28e-4ca9-b037-3f3618f2f636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e334e33-93fd-45e7-ab7f-6b68680ee012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14e0690-0625-4cea-9da7-a50e760468c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5a9f0-43f9-492f-bbed-b682e2c1b27e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2707f8-2874-451f-959f-b33edbd2d558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
